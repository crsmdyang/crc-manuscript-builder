"""
CRC Manuscript Builder â€“ Streamlit app (MVP v3.1.4)
Author: ChatGPT (for Jun)
Date: 2025-09-01 (KST)

ì´ë²ˆ ë²„ì „(v3.1.4) ë³€ê²½ì 
-------------------------
- âŒ **ModuleNotFoundError: 'docx' (python-docx)** í™˜ê²½ì—ì„œë„ ë™ìž‘í•˜ë„ë¡ **.docx ì½ê¸°/ì“°ê¸° ëŒ€ì²´ ê²½ë¡œ** ì¶”ê°€
  - ì½ê¸°: `python-docx`ê°€ ì—†ìœ¼ë©´ ZIPìœ¼ë¡œ `.docx`ë¥¼ ì—´ì–´ `word/document.xml`ì„ ì§ì ‘ íŒŒì‹±
  - ì“°ê¸°: `python-docx`ê°€ ì—†ìœ¼ë©´ **ìµœì†Œ DOCX**ë¥¼ ZIPìœ¼ë¡œ ìƒì„±(ë¬¸ë‹¨ë³„ í…ìŠ¤íŠ¸)
- âœ… v3.1.3ì˜ **Streamlit Shim**(UI ë¯¸ì„¤ì¹˜ í™˜ê²½)ê³¼ **PDF ë°±ì—”ë“œ ì¡°ê±´ë¶€ ìž„í¬íŠ¸** ìœ ì§€
- ðŸ§ª í…ŒìŠ¤íŠ¸ ë³´ê°•: DOCX fallback ìƒì„±/íŒŒì‹± ë¼ìš´ë“œíŠ¸ë¦½ í…ŒìŠ¤íŠ¸ ì¶”ê°€

ì„¤ì¹˜(ê¶Œìž¥)
----------
- UI ì‚¬ìš©(ê¶Œìž¥): `pip install streamlit requests pandas lxml pydantic tenacity python-docx pymupdf`
- ê²½ëŸ‰: `pip install streamlit requests pandas lxml pydantic tenacity pypdf`

ì‹¤í–‰:
  streamlit run app.py
"""
from __future__ import annotations
import os
import io
import re
import json
import zipfile
import requests
import pandas as pd
import xml.etree.ElementTree as ET
from typing import List, Dict, Optional, Tuple
from lxml import etree
from tenacity import retry, stop_after_attempt, wait_exponential
from pydantic import BaseModel, Field

# =====================
# Optional python-docx (with fallback)
# =====================
try:
    from docx import Document  # type: ignore
    from docx.shared import Pt  # type: ignore
    _HAVE_PYDOCX = True
except Exception:
    Document = None  # type: ignore
    Pt = None  # type: ignore
    _HAVE_PYDOCX = False

# =====================
# Streamlit import (with headless shim fallback)
# =====================
try:
    import streamlit as st  # type: ignore
    _HAVE_STREAMLIT = True
except Exception:
    _HAVE_STREAMLIT = False
    # ---- Streamlit Shim ----
    class _NoopContext:
        def __enter__(self):
            return self
        def __exit__(self, exc_type, exc, tb):
            return False
    class _SidebarShim:
        def success(self, *a, **k): print("[sidebar.success]", *a)
        def error(self, *a, **k): print("[sidebar.error]", *a)
        def warning(self, *a, **k): print("[sidebar.warning]", *a)
        def info(self, *a, **k): print("[sidebar.info]", *a)
    class _ColumnConfigShim:
        class CheckboxColumn:  # noqa: D401
            def __init__(self, *a, **k): pass
        class LinkColumn:
            def __init__(self, *a, **k): pass
        class TextColumn:
            def __init__(self, *a, **k): pass
    class _StreamlitShim:
        def __init__(self):
            self.session_state = {}
            self.sidebar = _SidebarShim()
            self.column_config = _ColumnConfigShim()
        # layout
        def set_page_config(self, *a, **k): pass
        def title(self, *a, **k): print("[title]", *a)
        def subheader(self, *a, **k): print("[subheader]", *a)
        def divider(self): print("[divider]")
        def caption(self, *a, **k): print("[caption]", *a)
        def write(self, *a, **k): print("[write]", *a)
        def markdown(self, *a, **k): print("[markdown]")
        def info(self, *a, **k): print("[info]", *a)
        def success(self, *a, **k): print("[success]", *a)
        def error(self, *a, **k): print("[error]", *a)
        def warning(self, *a, **k): print("[warning]", *a)
        # containers/contexts
        def expander(self, *a, **k): return _NoopContext()
        def spinner(self, *a, **k): return _NoopContext()
        def columns(self, spec):
            n = spec if isinstance(spec, int) else (len(spec) if hasattr(spec, "__len__") else 2)
            return [_NoopContext() for _ in range(n)]
        # widgets (return safe defaults)
        def text_area(self, *a, **k): return k.get("value", "")
        def text_input(self, *a, **k): return k.get("value", "")
        def selectbox(self, label, options, index=0, **k):
            try: return options[index]
            except Exception: return options[0] if options else ""
        def checkbox(self, label, value=False, **k): return bool(value)
        def slider(self, label, min_value=None, max_value=None, value=None, step=None, **k): return value
        def button(self, *a, **k): return False
        def file_uploader(self, *a, **k): return None
        def data_editor(self, df, *a, **k): return df
        def download_button(self, *a, **k): print("[download_button]")
    def cache_data(*a, **k):
        def _wrap(fn): return fn
        return _wrap
    st = _StreamlitShim()
    st.cache_data = cache_data

# =====================
# Conditional PDF backends (avoid 'frontend' import error from PyMuPDF)
# =====================
HAVE_FITZ = False
HAVE_PYPDF = False
_PDF_BACKEND = "none"
try:
    import fitz  # PyMuPDF
    HAVE_FITZ = True
    _PDF_BACKEND = "PyMuPDF"
except Exception:
    try:
        from pypdf import PdfReader as _PdfReader  # modern fork of PyPDF2
        HAVE_PYPDF = True
        _PDF_BACKEND = "pypdf"
    except Exception:
        try:
            from PyPDF2 import PdfReader as _PdfReader
            HAVE_PYPDF = True
            _PDF_BACKEND = "PyPDF2"
        except Exception:
            _PDF_BACKEND = "none"

# =====================
# Config & constants
# =====================
APP_TITLE = "CRC Manuscript Builder (MVP v3.1.4)"
EUTILS_BASE = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils"
CROSSREF_BASE = "https://api.crossref.org/works"
OPENALEX_BASE = "https://api.openalex.org/sources"
UNPAYWALL_BASE = "https://api.unpaywall.org/v2/"

MAX_RESULTS = 50
MAX_UPLOADS = 50

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
ENTREZ_API_KEY = os.getenv("ENTREZ_API_KEY")
UNPAYWALL_EMAIL = os.getenv("UNPAYWALL_EMAIL")

DOI_RE = re.compile(r"10\.\d{4,9}/[-._;()/:A-Za-z0-9]+")

# =====================
# Helpers
# =====================

def norm_text(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def find_doi_in_text(txt: str) -> Optional[str]:
    m = DOI_RE.search(txt or "")
    return m.group(0) if m else None


# ---------- DOCX fallback helpers ----------
_DOCX_NS = {
    "w": "http://schemas.openxmlformats.org/wordprocessingml/2006/main",
    "cp": "http://schemas.openxmlformats.org/package/2006/metadata/core-properties",
    "dc": "http://purl.org/dc/elements/1.1/",
    "dcterms": "http://purl.org/dc/terms/",
    "xsi": "http://www.w3.org/2001/XMLSchema-instance",
}


def read_docx_text_fallback(docx_bytes: bytes) -> str:
    """Read .docx text without python-docx by parsing word/document.xml."""
    try:
        with zipfile.ZipFile(io.BytesIO(docx_bytes)) as z:
            xml = z.read("word/document.xml")
        root = ET.fromstring(xml)
        texts = []
        for p in root.findall(".//w:p", _DOCX_NS):
            frag = []
            for t in p.findall(".//w:t", _DOCX_NS):
                frag.append(t.text or "")
            para = "".join(frag).strip()
            if para:
                texts.append(para)
        return "\n".join(texts)
    except Exception:
        return ""


def create_docx_from_markdown_fallback(md_text: str) -> bytes:
    """Create a minimal .docx (paragraph per block) without python-docx."""
    from xml.sax.saxutils import escape
    # very simple block split
    paragraphs = [p.strip() for p in md_text.split("\n\n")]
    if not any(paragraphs):
        paragraphs = [""]
    # Minimal required parts
    content_types = (
        """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Types xmlns="http://schemas.openxmlformats.org/package/2006/content-types">
  <Default Extension="rels" ContentType="application/vnd.openxmlformats-package.relationships+xml"/>
  <Default Extension="xml" ContentType="application/xml"/>
  <Override PartName="/word/document.xml" ContentType="application/vnd.openxmlformats-officedocument.wordprocessingml.document.main+xml"/>
  <Override PartName="/docProps/core.xml" ContentType="application/vnd.openxmlformats-package.core-properties+xml"/>
  <Override PartName="/docProps/app.xml" ContentType="application/vnd.openxmlformats-officedocument.extended-properties+xml"/>
</Types>"""
    ).strip()
    rels = (
        """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Relationships xmlns="http://schemas.openxmlformats.org/package/2006/relationships">
  <Relationship Id="rId1" Type="http://schemas.openxmlformats.org/officeDocument/2006/relationships/officeDocument" Target="word/document.xml"/>
</Relationships>"""
    ).strip()
    # Build document.xml paragraphs
    paras_xml = []
    for p in paragraphs:
        txt = escape(p)
        paras_xml.append(f"<w:p><w:r><w:t xml:space=\"preserve\">{txt}</w:t></w:r></w:p>")
    document_xml = (
        """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<w:document xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main">
  <w:body>
    {PARAS}
    <w:sectPr/>
  </w:body>
</w:document>""".replace("{PARAS}", "\n    ".join(paras_xml))
    ).strip()
    core = (
        """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<cp:coreProperties xmlns:cp="http://schemas.openxmlformats.org/package/2006/metadata/core-properties" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:dcmitype="http://purl.org/dc/dcmitype/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <dc:title>CRC Manuscript</dc:title>
  <dc:creator>CRC Manuscript Builder</dc:creator>
  <cp:lastModifiedBy>CRC Manuscript Builder</cp:lastModifiedBy>
</cp:coreProperties>"""
    ).strip()
    app = (
        """<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<Properties xmlns="http://schemas.openxmlformats.org/officeDocument/2006/extended-properties" xmlns:vt="http://schemas.openxmlformats.org/officeDocument/2006/docPropsVTypes">
  <Application>CRC Manuscript Builder</Application>
</Properties>"""
    ).strip()
    bio = io.BytesIO()
    with zipfile.ZipFile(bio, "w", zipfile.ZIP_DEFLATED) as z:
        z.writestr("[Content_Types].xml", content_types)
        z.writestr("_rels/.rels", rels)
        z.writestr("word/document.xml", document_xml)
        z.writestr("docProps/core.xml", core)
        z.writestr("docProps/app.xml", app)
    bio.seek(0)
    return bio.read()


# ---------- PDF extract ----------

def extract_pdf_text_and_doi(file_bytes: bytes) -> Tuple[str, Optional[str]]:
    """Extract text & DOI from PDF bytes using available backend.
    Returns (full_text, doi_or_None). Safe to call even if no backend.
    """
    # Backend 1: PyMuPDF
    if HAVE_FITZ:
        try:
            import fitz  # re-import inside for safety
            with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                texts = [page.get_text() for page in doc]
            full = "\n".join(texts)
            return full, find_doi_in_text(full)
        except Exception:
            pass  # fallthrough
    # Backend 2: pypdf / PyPDF2
    if HAVE_PYPDF:
        try:
            from io import BytesIO
            bio = BytesIO(file_bytes)
            reader = _PdfReader(bio)  # type: ignore[name-defined]
            texts = []
            for page in getattr(reader, "pages", []):
                try:
                    texts.append(page.extract_text() or "")
                except Exception:
                    texts.append("")
            full = "\n".join(texts)
            return full, find_doi_in_text(full)
        except Exception:
            pass
    # No backend
    return "", None


# =====================
# Data models
# =====================
class RefMeta(BaseModel):
    doi: Optional[str] = None
    title: Optional[str] = None
    journal: Optional[str] = None
    year: Optional[str] = None
    authors: List[str] = Field(default_factory=list)
    pmid: Optional[str] = None
    url: Optional[str] = None
    abstract_text: Optional[str] = None
    abstract_conclusion: Optional[str] = None


class ReferenceManager:
    def __init__(self):
        self.idx: Dict[str, int] = {}
        self.meta: Dict[str, RefMeta] = {}
        self.order: List[str] = []

    def _key(self, m: RefMeta) -> Optional[str]:
        if m.doi:
            return m.doi.lower()
        if m.pmid:
            return f"pmid:{m.pmid}"
        return None

    def register(self, meta: RefMeta) -> Optional[int]:
        key = self._key(meta)
        if not key:
            return None
        if key not in self.idx:
            self.order.append(key)
            self.idx[key] = len(self.order)
            self.meta[key] = meta
        else:
            old = self.meta.get(key)
            self.meta[key] = meta if len(json.dumps(meta.dict())) > len(json.dumps(old.dict())) else old
        return self.idx[key]

    def cite(self, doi_or_pmid: str) -> Optional[int]:
        key = doi_or_pmid.lower()
        if key not in self.idx:
            self.idx[key] = len(self.order) + 1
            self.order.append(key)
            self.meta.setdefault(key, RefMeta())
        return self.idx[key]

    def renumber_by_first_appearance(self, citation_sequence: List[str]):
        seen = []
        for key in citation_sequence:
            if key not in seen and key in self.idx:
                seen.append(key)
        for key in self.order:
            if key not in seen:
                seen.append(key)
        self.order = seen
        self.idx = {k: i + 1 for i, k in enumerate(self.order)}

    def vancouver(self, key: str) -> str:
        m = self.meta.get(key, RefMeta())
        def fmt_author(a: str) -> str:
            a = a.strip()
            parts = a.split()
            if not parts:
                return a
            last = parts[-1]
            initials = "".join(p[0] for p in parts[:-1] if p)
            return f"{last} {initials}".strip()
        authors = ", ".join(fmt_author(a) for a in (m.authors or [])[:6])
        if m.authors and len(m.authors) > 6:
            authors += ", et al."
        year = m.year or ""
        journal = m.journal or ""
        title = m.title or ""
        doi = f" https://doi.org/{m.doi}" if m.doi else (f" PMID:{m.pmid}" if m.pmid else "")
        return norm_text(f"{authors}. {title}. {journal}. {year}.{doi}")

    def render_reference_list(self) -> List[str]:
        return [self.vancouver(k) for k in self.order]


# =====================
# External services
# =====================
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=6))
def pubmed_search(term: str, retmax: int = MAX_RESULTS) -> List[str]:
    params = {"db": "pubmed", "term": term, "retmode": "json", "retmax": retmax, "sort": "relevance"}
    if ENTREZ_API_KEY:
        params["api_key"] = ENTREZ_API_KEY
    r = requests.get(f"{EUTILS_BASE}/esearch.fcgi", params=params, timeout=30)
    r.raise_for_status()
    data = r.json()
    return data.get("esearchresult", {}).get("idlist", [])


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=6))
def pubmed_fetch_xml(pmids: List[str]) -> etree._Element:
    if not pmids:
        return etree.Element("empty")
    params = {"db": "pubmed", "id": ",".join(pmids), "retmode": "xml"}
    if ENTREZ_API_KEY:
        params["api_key"] = ENTREZ_API_KEY
    r = requests.get(f"{EUTILS_BASE}/efetch.fcgi", params=params, timeout=60)
    r.raise_for_status()
    return etree.fromstring(r.content)


def _xml_text(node, xpath: str) -> Optional[str]:
    el = node.find(xpath)
    return norm_text(el.text) if el is not None and el.text else None


def _extract_abstract_and_conclusion(art: etree._Element) -> Tuple[Optional[str], Optional[str]]:
    nodes = art.findall(".//Abstract/AbstractText")
    if not nodes:
        return None, None
    parts = []
    conclusion = None
    for t in nodes:
        label = (t.get("Label") or t.get("NlmCategory") or "").lower()
        val = norm_text("".join(t.itertext()))
        if not val:
            continue
        parts.append(val)
        if label.startswith("conclusion"):
            sent = re.split(r"(?<=[.!?])\s+", val)
            conclusion = " ".join(sent[:2]).strip()
    full = " \n".join(parts) if parts else None
    if not conclusion and parts:
        sent = re.split(r"(?<=[.!?])\s+", parts[-1])
        conclusion = " ".join(sent[:2]).strip() if sent else None
    return full, conclusion


def pubmed_parse_records(root: etree._Element) -> List[RefMeta]:
    out: List[RefMeta] = []
    for art in root.findall(".//PubmedArticle"):
        pmid = _xml_text(art, ".//MedlineCitation/PMID")
        title = _xml_text(art, ".//Article/ArticleTitle")
        year = _xml_text(art, ".//Article/Journal/JournalIssue/PubDate/Year")
        journal = _xml_text(art, ".//Article/Journal/Title")
        # Authors
        authors = []
        for a in art.findall(".//AuthorList/Author"):
            ln = _xml_text(a, "LastName") or ""
            fn = _xml_text(a, "ForeName") or ""
            full = norm_text(f"{fn} {ln}")
            if full:
                authors.append(full)
        # DOI
        doi = None
        for idn in art.findall(".//ArticleIdList/ArticleId"):
            if idn.get("IdType") == "doi" and idn.text:
                doi = idn.text.lower()
        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/" if pmid else None
        abst, concl = _extract_abstract_and_conclusion(art)
        out.append(RefMeta(doi=doi, title=title, journal=journal, year=year, authors=authors,
                           pmid=pmid, url=url, abstract_text=abst, abstract_conclusion=concl))
    return out


def crossref_get(doi: str) -> Optional[RefMeta]:
    try:
        r = requests.get(f"{CROSSREF_BASE}/{doi}", timeout=30)
        if r.status_code != 200:
            return None
        j = r.json().get("message", {})
        title = "; ".join(j.get("title", [])) or None
        journal = (j.get("container-title") or [None])[0]
        year = None
        if j.get("issued", {}).get("'date-parts'"):
            year = str(j["issued"]["'date-parts'"][0][0])
        elif j.get("issued", {}).get("date-parts"):
            year = str(j["issued"]["date-parts"][0][0])
        authors = []
        for a in j.get("author", []):
            nm = norm_text(f"{a.get('given','')} {a.get('family','')}")
            if nm:
                authors.append(nm)
        url = j.get("URL")
        return RefMeta(doi=doi.lower(), title=title, journal=journal, year=year, authors=authors, url=url)
    except Exception:
        return None


def openalex_metric(journal_title: str) -> Optional[float]:
    try:
        q = {"search": journal_title}
        r = requests.get(OPENALEX_BASE, params=q, timeout=20)
        if r.status_code != 200:
            return None
        data = r.json().get("results", [])
        if not data:
            return None
        src = data[0]
        sjr2 = None
        try:
            sjr2 = src.get("summary_stats", {}).get("two_year_sjr")
        except Exception:
            sjr2 = None
        return float(sjr2) if sjr2 is not None else None
    except Exception:
        return None


def unpaywall_best_oa_link(doi: str) -> Optional[str]:
    if not UNPAYWALL_EMAIL:
        return None
    try:
        r = requests.get(f"{UNPAYWALL_BASE}{doi}", params={"email": UNPAYWALL_EMAIL}, timeout=20)
        if r.status_code != 200:
            return None
        j = r.json()
        rec = j.get("best_oa_location") or {}
        return rec.get("url")
    except Exception:
        return None


# =====================
# RIS exporter (EndNote)
# =====================

def to_ris(refs: List[RefMeta]) -> str:
    lines = []
    for r in refs:
        lines.append("TY  - JOUR")
        if r.title: lines.append(f"TI  - {r.title}")
        if r.journal: lines.append(f"JO  - {r.journal}")
        if r.year: lines.append(f"PY  - {r.year}")
        for au in (r.authors or []):
            lines.append(f"AU  - {au}")
        if r.doi: lines.append(f"DO  - {r.doi}")
        if r.url: lines.append(f"UR  - {r.url}")
        if r.pmid: lines.append(f"ID  - PMID:{r.pmid}")
        if r.abstract_text: lines.append(f"AB  - {r.abstract_text}")
        lines.append("ER  - ")
    return "\n".join(lines) + "\n"


# =====================
# UI (works in Streamlit; no-op in headless shim)
# =====================
st.set_page_config(page_title=APP_TITLE, layout="wide")
st.title(APP_TITLE)

with st.expander("ì‚¬ìš© ì§€ì¹¨(í•„ë…)", expanded=True):
    st.markdown(
        f"""
        - **í—ˆêµ¬ ê¸ˆì§€**: ì¸ìš©ì€ ì„ íƒ/ì—…ë¡œë“œí•œ ë¬¸í—Œìœ¼ë¡œë§Œ ì œí•œë©ë‹ˆë‹¤.
        - **IF ì •ë ¬**: `journal_if.csv` ì œê³µ ì‹œ IF ê¸°ì¤€ ì •ë ¬, ì—†ìœ¼ë©´ OpenAlex ì§€í‘œ(ì„ íƒ) ì‚¬ìš© ê°€ëŠ¥.
        - **RIS ë‚´ë³´ë‚´ê¸°**: ê²€ìƒ‰ ì„ íƒ/í—ˆìš© ë¬¸í—Œ(ì „ì²´Â·ì„ íƒ)ì„ EndNoteìš© `.ris`ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
        - **PDF ì—…ë¡œë“œ**: ìµœëŒ€ 50ê°œ, PDF ë‚´ DOI ìžë™ ì¶”ì¶œ ì‹œë„.
        - **ë°±ì—”ë“œ**: PDF â†’ **{_PDF_BACKEND}**, DOCX â†’ **{'python-docx' if _HAVE_PYDOCX else 'fallback-zip'}**, ëª¨ë“œ â†’ **{'Streamlit UI' if _HAVE_STREAMLIT else 'Headless(Shim)'}**
        """
    )

# 1) ìž…ë ¥(ì—°êµ¬ê³„íšì„œ .docx ì—…ë¡œë“œ í¬í•¨) + íƒ€ê¹ƒ ì €ë„/ìŠ¤íƒ€ì¼
colA, colB = st.columns([3, 2])
with colA:
    topic = st.text_area("ì£¼ì œ (Topic)", height=80)
    protocol = st.text_area("ì—°êµ¬ê³„íšì„œ ìš”ì•½ (Study Protocol)", height=160, key="protocol_ta")
    up_docx = st.file_uploader("ì—°êµ¬ê³„íšì„œ ìš”ì•½ .docx ì—…ë¡œë“œ (ì„ íƒ)", type=["docx"], accept_multiple_files=False)
    if up_docx is not None:
        try:
            data = up_docx.read()
            if _HAVE_PYDOCX and Document:
                bio = io.BytesIO(data)
                doc = Document(bio)
                text = "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
            else:
                text = read_docx_text_fallback(data)
            if text:
                st.session_state["protocol_ta"] = text
                st.success("ì›Œë“œ íŒŒì¼ì—ì„œ ì—°êµ¬ê³„íšì„œ ìš”ì•½ì„ ë¶ˆëŸ¬ì™”ì–´ìš”.")
            else:
                st.warning("ì›Œë“œ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ì›Œë“œ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
    results_txt = st.text_area("í•µì‹¬ ê²°ê³¼ ìš”ì•½ (Key Results)", height=100)

with colB:
    style_option = st.selectbox("íƒ€ê¹ƒ ì €ë„/ìŠ¤íƒ€ì¼", [
        "êµ­ë‚´: ëŒ€í•œëŒ€ìž¥í•­ë¬¸í•™íšŒ(Annals of Coloproctology)",
        "í•´ì™¸: ASCRS ìŠ¤íƒ€ì¼ (Diseases of the Colon & Rectum)",
        "í•´ì™¸: ESCP ìŠ¤íƒ€ì¼ (Colorectal Disease)",
        "ì—†ìŒ/ê¸°íƒ€(ì§ì ‘ìž…ë ¥)",
    ])
    custom_style = ""
    if style_option == "ì—†ìŒ/ê¸°íƒ€(ì§ì ‘ìž…ë ¥)":
        custom_style = st.text_input("ì§ì ‘ ìž…ë ¥", placeholder="ì˜ˆ: BJS ìŠ¤íƒ€ì¼, ë˜ëŠ” ëª©í‘œ ì €ë„ëª…")
    use_openalex = st.checkbox("OpenAlex ì§€í‘œ ì‚¬ìš©(ëŒ€ì²´ ì§€í‘œ)", value=False)

st.divider()

# 2) PubMed ê²€ìƒ‰
st.subheader("2) PubMed ê²€ìƒ‰")
search_query = st.text_input("ê²€ìƒ‰ì‹", placeholder="ì˜ˆ: (rectal cancer OR colorectal) AND (chemoradiation)")
retmax = st.slider("ê²€ìƒ‰ ê°œìˆ˜", 10, MAX_RESULTS, 50, step=5)
run_search = st.button("PubMed ê²€ìƒ‰ ì‹¤í–‰")

if "search_results" not in st.session_state:
    st.session_state["search_results"] = []
if "search_df" not in st.session_state:
    st.session_state["search_df"] = None

if run_search and search_query:
    with st.spinner("PubMed ê²€ìƒ‰ ì¤‘â€¦"):
        pmids = pubmed_search(search_query, retmax=retmax)
        root = pubmed_fetch_xml(pmids)
        st.session_state["search_results"] = pubmed_parse_records(root)

# 2-1) IF ë¶™ì´ê³  ì •ë ¬ (IF desc â†’ journal asc â†’ year desc)
@st.cache_data(show_spinner=False)
def load_journal_if_csv() -> Optional[pd.DataFrame]:
    try:
        if os.path.exists("journal_if.csv"):
            df = pd.read_csv("journal_if.csv")
            cols = {c.lower(): c for c in df.columns}
            jcol = cols.get("journal") or cols.get("title") or list(df.columns)[0]
            icol = cols.get("if") or cols.get("impact_factor") or list(df.columns)[1]
            df = df.rename(columns={jcol: "journal", icol: "if"})
            df["journal_norm"] = df["journal"].str.strip().str.lower()
            return df
    except Exception:
        pass
    return None

jif = load_journal_if_csv()

if st.session_state.get("search_results"):
    sdf = pd.DataFrame([
        {
            "select": False,
            "pmid": r.pmid,
            "doi": r.doi,
            "title": r.title,
            "journal": r.journal,
            "year": r.year,
            "Abstract": (r.abstract_text[:300] + "â€¦") if r.abstract_text and len(r.abstract_text) > 300 else (r.abstract_text or None),
            "Conclusion": r.abstract_conclusion,
            "url": r.url,
        }
        for r in st.session_state["search_results"]
    ])
    sdf["IF"] = None
    if jif is not None:
        jmap = dict(zip(jif["journal_norm"], jif["if"]))
        sdf["IF"] = sdf["journal"].fillna("").str.strip().str.lower().map(jmap)
    elif use_openalex:
        metrics = []
        for jn in sdf["journal"].fillna(""):
            metrics.append(openalex_metric(jn) if jn else None)
        sdf["IF"] = metrics

    def to_float(x):
        try: return float(x)
        except: return float("nan")
    def to_int(x):
        try: return int(x)
        except: return -1
    sdf["IF_num"] = sdf["IF"].apply(to_float).fillna(-1.0)
    sdf["year_num"] = sdf["year"].apply(to_int)
    sdf = sdf.sort_values(by=["IF_num", "journal", "year_num"], ascending=[False, True, False]).reset_index(drop=True)

    st.markdown("**ê²€ìƒ‰ ê²°ê³¼(ì²´í¬ â†’ í—ˆìš© ë¬¸í—Œ ì¶”ê°€ / RIS ë‚´ë³´ë‚´ê¸°)**")
    edited = st.data_editor(
        sdf,
        hide_index=True,
        use_container_width=True,
        column_config={
            "select": st.column_config.CheckboxColumn("ì„ íƒ"),
            "url": st.column_config.LinkColumn("PubMed"),
            "Abstract": st.column_config.TextColumn("Abstract", width="large"),
            "Conclusion": st.column_config.TextColumn("Conclusion", width="medium"),
            "IF": st.column_config.TextColumn("IF/Proxy"),
        },
        key="search_editor",
    )
    st.session_state["search_df"] = edited

    c1, c2, c3 = st.columns([1,1,2])
    with c1:
        add_sel = st.button("ì„ íƒ ì¶”ê°€ â†’ í—ˆìš© ë¬¸í—Œ")
    with c2:
        ris_sel_btn = st.button("ì„ íƒ .ris ë‹¤ìš´ë¡œë“œ")
    with c3:
        st.write(f"ì„ íƒ ìˆ˜: **{int((edited['select']==True).sum())}** íŽ¸")

    if add_sel or ris_sel_btn:
        chosen_rows = edited[edited["select"] == True]
        chosen_refs: List[RefMeta] = []
        for _, row in chosen_rows.iterrows():
            meta = RefMeta(
                doi=(str(row.get("doi")) if row.get("doi") else None),
                pmid=(str(row.get("pmid")) if row.get("pmid") else None),
                title=row.get("title"), journal=row.get("journal"), year=str(row.get("year")),
                url=row.get("url"), abstract_text=row.get("Abstract"), abstract_conclusion=row.get("Conclusion")
            )
            chosen_refs.append(meta)
            if add_sel:
                key = (meta.doi.lower() if meta.doi else (f"pmid:{meta.pmid}" if meta.pmid else None))
                if key:
                    st.session_state.setdefault("allowed", {})
                    st.session_state["allowed"][key] = meta
        if add_sel:
            st.success(f"í—ˆìš© ë¬¸í—Œì— {len(chosen_refs)}íŽ¸ ì¶”ê°€")
        if ris_sel_btn:
            ris_txt = to_ris(chosen_refs)
            st.download_button("ì„ íƒ .ris ë‹¤ìš´ë¡œë“œ", data=ris_txt.encode("utf-8"), file_name="pubmed_selection.ris", mime="application/x-research-info-systems")

st.divider()

# 3) PDF ì—…ë¡œë“œ â†’ í—ˆìš© ë¬¸í—Œ
st.subheader("3) PDF ì—…ë¡œë“œ(ìµœëŒ€ 50) â†’ í—ˆìš© ë¬¸í—Œ")
pdfs = st.file_uploader("ë…¼ë¬¸ PDF ì—…ë¡œë“œ", type=["pdf"], accept_multiple_files=True)
if st.button("PDFì—ì„œ DOI ì¶”ì¶œ í›„ ì¶”ê°€") and pdfs:
    if _PDF_BACKEND == "none":
        st.warning("PDF íŒŒì„œê°€ ì„¤ì¹˜ë˜ì–´ ìžˆì§€ ì•Šì•„ DOIë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'pymupdf' ë˜ëŠ” 'pypdf'ë¥¼ ì„¤ì¹˜í•´ ì£¼ì„¸ìš”.")
    added = 0
    st.session_state.setdefault("allowed", {})
    for up in pdfs[:MAX_UPLOADS]:
        content = up.read()
        _, doi = extract_pdf_text_and_doi(content)
        if doi:
            meta = crossref_get(doi) or RefMeta(doi=doi)
            st.session_state["allowed"][doi.lower()] = meta
            added += 1
    st.success(f"PDFì—ì„œ {added}íŽ¸ ì¶”ê°€ (DOI ë¯¸íƒì§€ íŒŒì¼ì€ ìƒëžµ)")

# 4) í—ˆìš© ë¬¸í—Œ í‘œ(ì¶”ê°€/ì‚­ì œ, RIS ë‚´ë³´ë‚´ê¸°)
st.subheader("4) í—ˆìš© ë¬¸í—Œ (ì¸ìš© ê°€ëŠ¥í•œ ì§‘í•©)")
st.session_state.setdefault("allowed", {})

if st.session_state.get("allowed"):
    adf_rows = []
    for k, v in st.session_state["allowed"].items():
        # ë°©ì–´ì  ì ‘ê·¼(í˜¹ì‹œ dict ë“±ì´ ì„žì—¬ ë“¤ì–´ì˜¨ ê²½ìš° ëŒ€ë¹„)
        doi = getattr(v, "doi", None)
        pmid = getattr(v, "pmid", None)
        title = getattr(v, "title", None)
        journal = getattr(v, "journal", None)
        year = getattr(v, "year", None)
        abstract_text = getattr(v, "abstract_text", None)
        conclusion_text = getattr(v, "abstract_conclusion", None)
        preview = (abstract_text[:300] + "â€¦") if abstract_text and len(abstract_text) > 300 else (abstract_text or None)
        adf_rows.append({
            "select": False,
            "key": k,
            "doi": doi,
            "pmid": pmid,
            "title": title,
            "journal": journal,
            "year": year,
            "Abstract": preview,
            "Conclusion": conclusion_text,
            "OA_link": unpaywall_best_oa_link(doi) if doi else None,
        })
    adf = pd.DataFrame(adf_rows)

    edited_allowed = st.data_editor(
        adf,
        hide_index=True,
        use_container_width=True,
        column_config={
            "select": st.column_config.CheckboxColumn("ì„ íƒ"),
            "OA_link": st.column_config.LinkColumn("OA ë§í¬"),
            "Abstract": st.column_config.TextColumn("Abstract", width="large"),
            "Conclusion": st.column_config.TextColumn("Conclusion", width="medium"),
        },
        key="allowed_editor",
    )

    c1, c2, c3 = st.columns([1,1,2])
    with c1:
        del_btn = st.button("ì„ íƒ ì‚­ì œ")
    with c2:
        ris_allowed_sel = st.button("ì„ íƒ .ris ë‹¤ìš´ë¡œë“œ")
    with c3:
        ris_allowed_all = st.button("ì „ì²´ .ris ë‹¤ìš´ë¡œë“œ")

    if del_btn:
        to_del = edited_allowed[edited_allowed["select"] == True]["key"].tolist()
        for k in to_del:
            st.session_state["allowed"].pop(k, None)
        st.success(f"ì‚­ì œ ì™„ë£Œ: {len(to_del)}íŽ¸")

    if ris_allowed_sel or ris_allowed_all:
        export_keys = (edited_allowed[edited_allowed["select"] == True]["key"].tolist() if ris_allowed_sel
                       else list(st.session_state["allowed"].keys()))
        refs = [st.session_state["allowed"][k] for k in export_keys if k in st.session_state["allowed"]]
        ris_txt = to_ris(refs)
        fname = "allowed_selection.ris" if ris_allowed_sel else "allowed_all.ris"
        st.download_button(".ris ë‹¤ìš´ë¡œë“œ", data=ris_txt.encode("utf-8"), file_name=fname, mime="application/x-research-info-systems")
else:
    st.info("í—ˆìš© ë¬¸í—Œì´ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì„ íƒí•˜ì—¬ ì¶”ê°€í•˜ì„¸ìš”.")

st.divider()

# 5) ì„¹ì…˜ë³„ ìƒì„± â†’ ë³‘í•©
st.subheader("5) ì„¹ì…˜ë³„ ìƒì„± â†’ ìµœì¢… ë³‘í•©")
class LLM:
    def __init__(self, model: str, api_key: Optional[str]):
        self.model = model
        self.key = api_key
        self.enabled = bool(api_key)
        if not self.enabled:
            st.warning("OPENAI_API_KEY ê°€ ì„¤ì •ë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒì„± ê¸°ëŠ¥ì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

    def generate_section(self, section: str, topic: str, protocol: str, results: str,
                         allowed_refs: Dict[str, RefMeta], style_note: str) -> str:
        if not self.enabled:
            return "(LLM ë¹„í™œì„±í™”: OPENAI_API_KEY ì„¤ì • í•„ìš”)"
        try:
            refs_serialized = []
            for k, m in allowed_refs.items():
                label = m.doi or m.pmid or k
                refs_serialized.append({
                    "key": k,
                    "label": label,
                    "title": m.title,
                    "journal": m.journal,
                    "year": m.year,
                })
            system = (
                "You are an evidence-based medical writing assistant for colorectal surgery/oncology. "
                "Strictly use only the provided source list and cite with [CITE:DOI] or [CITE:pmid:ID]. "
                "If evidence is insufficient, write 'No high-quality evidence available.' Do not invent citations."
            )
            user = {
                "task": f"Write the {section} for a colorectal manuscript.",
                "topic": topic,
                "study_protocol": protocol,
                "key_results": results,
                "journal_style": style_note,
                "citation_rule": "Use only allowed sources via [CITE:...] tags.",
                "allowed_sources": refs_serialized,
                "language": "Korean",
            }
            payload = {"model": self.model, "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": json.dumps(user, ensure_ascii=False)},
            ], "temperature": 0.2}
            r = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={"Authorization": f"Bearer {self.key}", "Content-Type": "application/json"},
                json=payload, timeout=120,
            )
            r.raise_for_status()
            content = r.json()["choices"][0]["message"]["content"]
            bad = []
            for tag in re.findall(r"\[CITE:([^\]]+)\]", content or ""):
                k = tag.strip().lower()
                if k not in allowed_refs:
                    bad.append(k)
            if bad:
                return (
                    "(ìƒì„± ê±°ë¶€) í—ˆìš©ë˜ì§€ ì•Šì€ ì¸ìš© íƒœê·¸: " + ", ".join(sorted(set(bad))) + "\ní—ˆìš©ëœ DOI/PMIDë§Œ ì‚¬ìš©í•˜ì„¸ìš”."
                )
            return content
        except Exception as e:
            return f"(LLM ì˜¤ë¥˜) {e}"

llm = LLM(OPENAI_MODEL, OPENAI_API_KEY)
style_note = (custom_style if style_option == "ì—†ìŒ/ê¸°íƒ€(ì§ì ‘ìž…ë ¥)" else style_option)
SECTIONS = ["Cover Letter", "Title Page", "Abstract", "Introduction", "Methods", "Results", "Discussion"]

if "sections" not in st.session_state:
    st.session_state["sections"] = {}

cols = st.columns(2)
for i, sec in enumerate(SECTIONS):
    with cols[i % 2]:
        st.markdown(f"**{sec}**")
        if st.button(f"{sec} ìƒì„±", key=f"gen_{sec}"):
            txt = llm.generate_section(sec, topic, st.session_state.get("protocol_ta", ""), results_txt, st.session_state.get("allowed", {}), style_note)
            st.session_state["sections"][sec] = txt
        st.text_area(f"{sec} ë¯¸ë¦¬ë³´ê¸°", value=st.session_state["sections"].get(sec, ""), height=200, key=f"ta_{sec}")

st.markdown("**References** ì„¹ì…˜ì€ ìµœì¢… ë³‘í•© ë‹¨ê³„ì—ì„œ ìžë™ ìƒì„±ë©ë‹ˆë‹¤.")

if st.button("ìµœì¢… ë³‘í•© ë° ë²ˆí˜¸ ìž¬ì •ë ¬"):
    rm = ReferenceManager()
    for k, m in st.session_state.get("allowed", {}).items():
        if k.startswith("pmid:") and m:
            m.pmid = k.split(":",1)[1]
        elif m:
            m.doi = m.doi or k
        rm.register(m)

    def replace_citations(text: str) -> Tuple[str, List[str]]:
        seq = []
        def _rep(m):
            tag = m.group(1).strip().lower()
            if tag not in st.session_state.get("allowed", {}):
                return f"[CITE-INVALID:{tag}]"
            seq.append(tag)
            n = rm.cite(tag)
            return f"[{n}]"
        new = re.sub(r"\[CITE:([^\]]+)\]", _rep, text or "")
        return new, seq

    merged_parts = []
    citation_seq = []
    for sec in SECTIONS:
        txt = st.session_state["sections"].get(sec, "")
        if not txt:
            continue
        rep, seq = replace_citations(txt)
        merged_parts.append(f"## {sec}\n\n" + rep.strip())
        citation_seq.extend(seq)

    rm.renumber_by_first_appearance(citation_seq)
    refs = rm.render_reference_list()

    final_md = "\n\n".join(merged_parts) + "\n\n## References\n\n" + "\n".join(
        [f"[{i+1}] {line}" for i, line in enumerate(refs)]
    )
    st.session_state["final_md"] = final_md
    st.success("ë³‘í•© ì™„ë£Œ â€“ ì•„ëž˜ì—ì„œ ë¯¸ë¦¬ë³´ê¸°/ë‚´ë³´ë‚´ê¸° í•˜ì„¸ìš”.")

# ë¯¸ë¦¬ë³´ê¸° ë° ë‚´ë³´ë‚´ê¸°
if "final_md" in st.session_state:
    st.subheader("ë¯¸ë¦¬ë³´ê¸° (Markdown)")
    st.text_area("Final Markdown", value=st.session_state["final_md"], height=420)

    def md_to_docx_bytes(md_text: str) -> bytes:
        if _HAVE_PYDOCX and Document and Pt:
            # python-docx path
            bio = io.BytesIO()
            try:
                doc = Document()
                style = doc.styles["Normal"]
                style.font.name = "Calibri"
                style.font.size = Pt(11)
                for block in md_text.split("\n\n"):
                    doc.add_paragraph(block)
                doc.save(bio)
                bio.seek(0)
                return bio.read()
            except Exception:
                # fallback if something goes wrong even with python-docx
                return create_docx_from_markdown_fallback(md_text)
        else:
            # fallback builder
            return create_docx_from_markdown_fallback(md_text)

    st.download_button("Download .md", data=st.session_state["final_md"].encode("utf-8"), file_name="manuscript.md", mime="text/markdown")
    st.download_button("Download .docx", data=md_to_docx_bytes(st.session_state["final_md"]), file_name="manuscript.docx", mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document")

st.divider()

st.caption("Â© 2025 CRC Manuscript Builder (MVP v3.1.4). Evidence-locked generation. No PHI.")

# =====================
# (Optional) Lightweight sanity tests (run only if explicitly requested)
# =====================

def _run_sanity_tests():
    """Minimal non-network tests to catch regressions in critical blocks."""
    # 1) Test RIS exporter with minimal RefMeta (existing)
    r = RefMeta(doi="10.1000/test.doi", title="T", journal="J", year="2024", authors=["A B"], pmid="1", url="U", abstract_text="abs")
    ris = to_ris([r])
    assert "TY  - JOUR" in ris and "DO  - 10.1000/test.doi" in ris
    # 2) Test reference manager numbering (existing)
    rm = ReferenceManager(); rm.register(r); n = rm.cite(r.doi)
    assert n == 1
    # 3) Test citation replacement helper (existing)
    txt = "See [CITE:10.1000/test.doi]."
    rm2 = ReferenceManager(); rm2.register(r)
    def _rep(m):
        tag = m.group(1).strip().lower(); rm2.cite(tag); return "[1]"
    out = re.sub(r"\[CITE:([^\]]+)\]", _rep, txt)
    assert out == "See [1]."
    # 4) New: DOI regex basic
    assert find_doi_in_text("doi:10.5555/abc.DEF-123") == "10.5555/abc.DEF-123"
    # 5) New: whitespace normalization
    assert norm_text("  a\t b\n c  ") == "a b c"
    # 6) New: PDF extractor with invalid bytes should not crash and return (text, None)
    text, doi = extract_pdf_text_and_doi(b"not-a-real-pdf")
    assert isinstance(text, str) and (doi is None or isinstance(doi, str))
    # 7) New: DOCX fallback roundtrip (build then read)
    sample = "Hello world\n\nSecond paragraph"
    built = create_docx_from_markdown_fallback(sample)
    parsed = read_docx_text_fallback(built)
    assert "Hello world" in parsed and "Second paragraph" in parsed
    return "OK"

# To run: set environment variable RUN_APP_TESTS=1 before launching Streamlit
if os.getenv("RUN_APP_TESTS") == "1":
    try:
        res = _run_sanity_tests()
        st.sidebar.success(f"Sanity tests: {res}")
    except Exception as e:
        st.sidebar.error(f"Sanity tests failed: {e}")
